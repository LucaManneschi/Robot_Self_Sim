{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49a92f8-78e8-40f4-a6fd-c7e65559df54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "torch.cuda.is_available()\n",
    "\n",
    "import time\n",
    "\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import os\n",
    "from scipy import io\n",
    "import pickle\n",
    "device='cuda'\n",
    "torch.set_default_tensor_type(torch.FloatTensor)\n",
    "\n",
    "#from matplotlib import cm\n",
    "import torchvision\n",
    "\n",
    "\n",
    "import sys\n",
    "#inp = int(float(sys.argv[1])-1)\n",
    "inp=0\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4f68ec-32ff-454c-bea4-37916f6ba267",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data_Manager(nn.Module):\n",
    "    \n",
    "    def __init__(self,tr_prc=0.8,Dataset=1, Black_and_White=False):\n",
    "        super().__init__()\n",
    "\n",
    "        if Dataset==1:\n",
    "        \n",
    "            self.Y=torch.tensor(np.load('../../Robot Self Dataset New/data_hands_closer/states.npy').astype(\"float32\"),device='cpu').to(torch.float16)\n",
    "            self.A=torch.tensor(np.load('../../Robot Self Dataset New/data_hands_closer/actions.npy').astype(\"float32\"),device='cpu').to(torch.float16)\n",
    "            \n",
    "            N_files=10\n",
    "            self.X=[]\n",
    "            self.L=[]\n",
    "            \n",
    "            for n in range(0,N_files):\n",
    "    \n",
    "                title_file='../../Robot Self Dataset New/data_hands_closer/images-'+str(n)+'.npz'\n",
    "                \n",
    "                Image=np.load(title_file)\n",
    "                \n",
    "                ## mean Converts to black and white\n",
    "                #self.X.append( torch.mean(torch.tensor(Image['arr_0'][1,...]).to(torch.float16),3,keepdim=True) )\n",
    "                self.X.append( torch.tensor(Image['arr_0'][1,...]).to(torch.uint8) )\n",
    "\n",
    "                title_file='../../Robot Self Dataset New/data_hands_closer/labels-'+str(n)+'.npz'\n",
    "                Labels=np.load(title_file)\n",
    "\n",
    "                if n>=int(tr_prc*10):\n",
    "                    self.L.append( (torch.tensor(Labels['arr_0'])[1,:,:,:,1,:]).unsqueeze(-2).to(torch.uint8) )\n",
    "\n",
    "            self.X=torch.stack(self.X,0).reshape([-1, self.X[0].size()[1], self.X[0].size()[2], self.X[0].size()[3], self.X[0].size()[4] ])\n",
    "            self.X=self.X.permute(0,3,1,2,4)\n",
    "            \n",
    "            self.N_seq=self.X.size()[0]\n",
    "            self.N_tr=int(tr_prc*self.N_seq)\n",
    "            self.N_val=int( (self.N_seq-self.N_tr)/2 )\n",
    "            \n",
    "                        \n",
    "            self.L=torch.stack(self.L,0).reshape([-1, self.L[0].size()[1], self.L[0].size()[2], self.L[0].size()[3], self.L[0].size()[4] ])\n",
    "            self.L=self.L.permute(0,3,1,2,4)\n",
    "\n",
    "            \n",
    "            \n",
    "\n",
    "        if Dataset==2:\n",
    "\n",
    "            self.Y=torch.tensor(np.load('../../Data_Tiny_Catch/data_1000/states.npy')).to(torch.float16)\n",
    "            self.A=torch.tensor(np.load('../../Data_Tiny_Catch/data_1000/actions.npy')).to(torch.float16)\n",
    "            \n",
    "            N_files=10\n",
    "            Images=[]\n",
    "            self.X=[]\n",
    "            \n",
    "            for n in range(1,N_files+1):\n",
    "                \n",
    "                title_file='../../Data_Tiny_Catch/data_1000/images-'+str(n)+'.npz'\n",
    "                \n",
    "                Image=np.load(title_file)\n",
    "                self.X.append(torch.tensor(Image['arr_0']))\n",
    "            \n",
    "            self.X=torch.stack(self.X,0).reshape([-1, self.X[0].size()[1], self.X[0].size()[2], self.X[0].size()[3], self.X[0].size()[4] ])\n",
    "            self.X=self.X.permute(0,3,1,2,4)\n",
    "\n",
    "        self.X_M=torch.max(self.X)\n",
    "\n",
    "        self.N_ch=self.X.size()[1]\n",
    "        self.T_flow=10\n",
    "        self.alpha=1\n",
    "\n",
    "        self.ema_kernel=torch.zeros([1,self.T_flow,1,1],device=device)\n",
    "        self.ema_kernel[0,:,0,0]=(torch.pow((1-self.alpha),torch.arange(0,self.T_flow))*self.alpha)\n",
    "        \n",
    "        self.Compute_Flow()\n",
    "        \n",
    "        self.Hei=self.X.size()[1]\n",
    "        self.Len=self.X.size()[2]\n",
    "        self.T=self.X.size()[-1]\n",
    "        \n",
    "        \n",
    "        self.X_tr_flow=self.X_flow[0:self.N_tr,...]\n",
    "        self.X_tr=self.X[0:self.N_tr,...]\n",
    "        \n",
    "        self.X_te_flow=self.X_flow[self.N_tr:,...]\n",
    "        self.X_te=self.X[self.N_tr:,...]\n",
    "        \n",
    "        self.Y_tr=self.Y[0:self.N_tr,...]\n",
    "        self.Y_te=self.Y[self.N_tr:,...]\n",
    "\n",
    "        self.A_tr=self.A[0:self.N_tr,...]\n",
    "        self.A_te=self.A[self.N_tr:,...]\n",
    "\n",
    "        self.resolution=self.X_tr.size()[-2]\n",
    "\n",
    "    def Compute_Flow(self):\n",
    "\n",
    "        T=self.X.size()[-1]\n",
    "\n",
    "        self.X_flow=torch.zeros([self.X.size()[0],1,self.X.size()[2],self.X.size()[3],self.X.size()[4]],dtype=torch.uint8)\n",
    "        \n",
    "        for t in range(1,T):\n",
    "\n",
    "            x2=self.X[...,t].to(torch.uint8)/self.X_M\n",
    "            x1=self.X[...,t-1].to(torch.uint8)/self.X_M\n",
    "\n",
    "            delta=(torch.mean(x2-x1,1,keepdim=True))\n",
    "            \n",
    "            self.X_flow[...,t]=(1-self.alpha)*self.X_flow[...,t-1]+self.alpha*(delta+1)/2\n",
    "\n",
    "\n",
    "    def Normalise(self):\n",
    "\n",
    "        self.m=self.Y_tr.mean(0).unsqueeze(0)\n",
    "        self.std=torch.sqrt(self.Y_tr.var(0).unsqueeze(0))\n",
    "        \n",
    "        epsilon=10**(-5)\n",
    "        \n",
    "        self.Y_tr=(self.Y_tr-self.m)/(self.std+epsilon)        \n",
    "        self.Y_te=(self.Y_te-self.m)/(self.std+epsilon)\n",
    "\n",
    "        self.m_a=self.A_tr.mean(0).unsqueeze(0)\n",
    "        self.std_a=torch.sqrt(self.A_tr.var(0).unsqueeze(0))\n",
    "        \n",
    "        self.A_tr=(self.A_tr-self.m_a)/(self.std_a+epsilon)\n",
    "        self.A_te=(self.A_te-self.m_a)/(self.std_a+epsilon)\n",
    "\n",
    "\n",
    "    def Unnormalise(self, Y, Y_true):\n",
    "\n",
    "        Y=Y*(self.std+epsilon)+self.m\n",
    "        Y_true=Y_true*(self.std+epsilon)+self.m\n",
    "\n",
    "        return Y, Y_true\n",
    "    \n",
    "    def Batch(self, batch_size):\n",
    "\n",
    "        rand_ind=torch.randint(0,self.X_tr.size()[0], [batch_size])\n",
    "        rand_t=torch.randint(0,self.X_tr.size()[-1], [batch_size])\n",
    "        \n",
    "        x_batch=self.X_tr[rand_ind,:,:,:,rand_t].float().to(device)/self.X_M\n",
    "        x_batch_flow=self.X_tr_flow[rand_ind,:,:,:,rand_t].float().to(device)\n",
    "        \n",
    "        x_batch=torch.concat([x_batch,x_batch_flow],1)\n",
    "        y_batch=self.Y_tr[rand_ind,:,rand_t].float().to(device)\n",
    "        \n",
    "        return x_batch, y_batch\n",
    "\n",
    "    def Evaluate(self, batch_size_te):\n",
    "\n",
    "        rand_ind=torch.randint(0,self.X_te.size()[0], [batch_size_te])\n",
    "        rand_t=torch.randint(1,self.X_te.size()[-1], [batch_size_te])\n",
    "\n",
    "        x_batch=self.X_te[rand_ind,:,:,:,rand_t].float().to(device)/self.X_M\n",
    "        x_batch_flow=self.X_te_flow[rand_ind,:,:,:,rand_t].float().to(device)\n",
    "        l_batch=self.L[rand_ind,:,:,:,rand_t].to(device)\n",
    "        \n",
    "        x_batch=torch.concat([x_batch,x_batch_flow],1)\n",
    "        y_batch=self.Y_te[rand_ind,:,rand_t].float().to(device)\n",
    "\n",
    "        return x_batch, y_batch, l_batch\n",
    "\n",
    "\n",
    "    def Batch_ODE(self, batch_size, T_horizon):\n",
    "        \n",
    "        A=torch.zeros([batch_size,self.A.size()[1],T_horizon],device=device)\n",
    "        \n",
    "        y=torch.zeros([batch_size,self.Y_tr.size()[1],T_horizon],device=device)\n",
    "        x=torch.zeros([batch_size,T_horizon,self.X_tr.size()[1]+1,self.X_tr.size()[2],self.X_tr.size()[3]],device=device)\n",
    "        \n",
    "        x0s=torch.zeros([batch_size,self.Y_tr.size()[1]],device=device)\n",
    "        \n",
    "        rand_ind=torch.randint(0, self.Y_tr.size()[0], [batch_size])\n",
    "        t_rand=torch.randint(0, self.Y_tr.size()[2]-T_horizon, [batch_size])\n",
    "        \n",
    "        for n in range(batch_size):\n",
    "            \n",
    "            A[n,:,:]=self.A_tr[rand_ind[n],:,t_rand[n]:t_rand[n]+T_horizon].float().to(device)\n",
    "\n",
    "            x[n,:,0:self.N_ch,:,:]=self.X_tr[rand_ind[n],:,:,:,t_rand[n]:t_rand[n]+T_horizon].unsqueeze(0).transpose(-1,0).squeeze().float().to(device)/self.X_M\n",
    "            x[n,:,-1,:,:]=self.X_tr_flow[rand_ind[n],:,:,:,t_rand[n]:t_rand[n]+T_horizon].unsqueeze(0).transpose(-1,0).squeeze().float().to(device)\n",
    "\n",
    "            y[n,:,:]=self.Y_tr[rand_ind[n],:,t_rand[n]:t_rand[n]+T_horizon].float().to(device)\n",
    "            \n",
    "            x0s[n,:]=self.Y_tr[rand_ind[n],:,t_rand[n]].float().to(device)\n",
    "        \n",
    "        t0s=t_rand.to(device)\n",
    "        \n",
    "        return x, y, A, x0s, t0s\n",
    "\n",
    "    def Evaluate_ODE(self, batch_size, T_horizon_val):\n",
    "        \n",
    "        A=torch.zeros([batch_size,self.A_te.size()[1],T_horizon_val],device=device)\n",
    "        \n",
    "        y=torch.zeros([batch_size,self.Y_te.size()[1],T_horizon_val],device=device)\n",
    "        x=torch.zeros([batch_size,T_horizon_val,self.X_te.size()[1]+1,self.X_te.size()[2],self.X_te.size()[3]],device=device)\n",
    "        l=torch.zeros([batch_size,T_horizon_val,self.L.size()[1],self.L.size()[2],self.L.size()[3]],device=device,dtype=torch.uint8)\n",
    "\n",
    "        x0s=torch.zeros([batch_size,self.Y_te.size()[1]],device=device)\n",
    "        \n",
    "        rand_ind=torch.randint(0, self.Y_te.size()[0], [batch_size],device=device)\n",
    "        t_rand=torch.randint(1, self.Y_te.size()[2]-T_horizon_val, [batch_size],device=device)\n",
    "        \n",
    "        for n in range(batch_size):\n",
    "            \n",
    "            A[n,:,:]=self.A_te[rand_ind[n],:,t_rand[n]:t_rand[n]+T_horizon_val].float().to(device)\n",
    "\n",
    "            x[n,:,0:self.N_ch,:,:]=self.X_te[rand_ind[n],:,:,:,t_rand[n]:t_rand[n]+T_horizon_val].unsqueeze(0).transpose(-1,0).squeeze().float().to(device)/self.X_M\n",
    "            x[n,:,-1,:,:]=self.X_te_flow[rand_ind[n],:,:,:,t_rand[n]:t_rand[n]+T_horizon_val].unsqueeze(0).transpose(-1,0).squeeze().float().to(device)\n",
    "            l[n,:,0:self.N_ch,:,:]=self.L[rand_ind[n],:,:,:,t_rand[n]:t_rand[n]+T_horizon_val].unsqueeze(0).transpose(-1,0).squeeze(-1).to(device)\n",
    "            \n",
    "            y[n,:,:]=self.Y_te[rand_ind[n],:,t_rand[n]:t_rand[n]+T_horizon_val].float().to(device)\n",
    "            \n",
    "            x0s[n,:]=self.Y_te[rand_ind[n],:,t_rand[n]].float().to(device)\n",
    "        \n",
    "        t0s=t_rand.to(device)\n",
    "        \n",
    "        return x, y, A, x0s, t0s, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655e0787-0b03-455b-bdf3-c0417a8b7234",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data=Data_Manager()\n",
    "\n",
    "batch_size=30\n",
    "T_horizon=50\n",
    "\n",
    "x_batch, y_batch=Data.Batch(batch_size)\n",
    "x_b, y_b, A_b, x0s, t0s=Data.Batch_ODE(batch_size, T_horizon=T_horizon)\n",
    "x_te, y_te, A_te, x0s, t0s, l_te=Data.Evaluate_ODE(batch_size, T_horizon_val=T_horizon)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a07e230-dd53-4ca8-9531-4b8e4d4b8901",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd01947c-384f-46d4-a51e-acf3d6adad0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb64fd7-11e4-460d-aeaf-3a93a0065434",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1, dropout=0.):\n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.conv1=nn.Conv2d(in_ch, out_ch, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1=nn.BatchNorm2d(out_ch)\n",
    "        self.relu=nn.ReLU()\n",
    "\n",
    "        self.conv2=nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.downsample = None\n",
    "        if in_ch != out_ch:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d( in_ch, out_ch, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        identity = self.downsample(x) if self.downsample else x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        out = self.relu(x+identity)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self, Ch):\n",
    "        super(ResNet, self).__init__()\n",
    "\n",
    "        self.Ch=Ch\n",
    "        self.First_Block=[]\n",
    "        \n",
    "        self.First_Block.append( nn.Conv2d(Ch[0], Ch[1], kernel_size=3, stride=1, padding=1, bias=False) )\n",
    "        self.First_Block.append( nn.BatchNorm2d(Ch[1]) )\n",
    "        self.First_Block.append( nn.ReLU() )\n",
    "        self.First_Block.append( nn.MaxPool2d(kernel_size=3, stride=2, padding=1) )\n",
    "\n",
    "        self.First_Block=torch.nn.Sequential(*self.First_Block).to(device)\n",
    "        \n",
    "        self.layer=[]\n",
    "        self.layer.append( ResBlock(Ch[1], Ch[2], stride=1) )\n",
    "        \n",
    "        for n in range(3,len(Ch)):\n",
    "\n",
    "            self.layer.append( ResBlock(Ch[n-1], Ch[n], stride=2) ) \n",
    "\n",
    "        self.layer=torch.nn.Sequential(*self.layer).to(device)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x=self.First_Block(x)\n",
    "        out=self.layer(x)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class ResBlockTranspose(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, stride=1, dropout=0.):\n",
    "        super(ResBlockTranspose, self).__init__()\n",
    "        \n",
    "        self.up = (stride == 2)\n",
    "        \n",
    "        self.conv1 = nn.ConvTranspose2d(in_ch, out_ch, kernel_size=3, stride=stride,\n",
    "                                        padding=1, output_padding=1 if self.up else 0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_ch)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(out_ch, out_ch, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_ch)\n",
    "\n",
    "        self.upsample = None\n",
    "        if in_ch != out_ch or self.up:\n",
    "            self.upsample = nn.Sequential(\n",
    "                nn.ConvTranspose2d(in_ch, out_ch, kernel_size=1, stride=stride,\n",
    "                                   output_padding=1 if self.up else 0, bias=False),\n",
    "                nn.BatchNorm2d(out_ch)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = self.upsample(x) if self.upsample else x\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        out = self.relu(x + identity)\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet_Transpose(nn.Module):\n",
    "    def __init__(self, Ch):\n",
    "        super(ResNet_Transpose, self).__init__()\n",
    "\n",
    "        self.layers = []\n",
    "\n",
    "        # Reverse the channel sequence from encoder: [256, 128, 64, 3]\n",
    "        rev_channels = Ch[::-1]\n",
    "\n",
    "        self.layers.append(ResBlockTranspose(rev_channels[0], rev_channels[1], stride=2))\n",
    "        for i in range(1, len(rev_channels) - 2):\n",
    "            self.layers.append(ResBlockTranspose(rev_channels[i], rev_channels[i+1], stride=2))\n",
    "        \n",
    "        # Final upsample block (optional: back to original resolution)\n",
    "        self.layers.append(nn.ConvTranspose2d(rev_channels[-2], rev_channels[-1],\n",
    "                                              kernel_size=3, stride=1, padding=1))\n",
    "\n",
    "        self.net = nn.Sequential(*self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ead3b6b-a841-4dbd-bc0e-0578da14d6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, resolution, cnn_dim, latent_dim, sensor_dim, CUT=False):\n",
    "        super().__init__()\n",
    "\n",
    "        Chs=[4,16,32,64]\n",
    "        cnn_dim=Chs[-1]\n",
    "\n",
    "        self.encoder_visual_self = ResNet(Chs).to(device)\n",
    "        self.encoder_visual_other = ResNet(Chs).to(device)\n",
    "        \n",
    "        self.latent_dim=latent_dim\n",
    "        self.resolution=resolution\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=1)\n",
    "        \n",
    "        self.project_self = torch.nn.Sequential(nn.Linear(cnn_dim * (resolution // 4) ** 2, 2*latent_dim),\n",
    "                                                nn.ReLU(), \n",
    "                                                nn.Linear(2*latent_dim, latent_dim))\n",
    "        \n",
    "        self.project_other = torch.nn.Sequential(nn.Linear(cnn_dim * (resolution // 4) ** 2, 2*latent_dim),\n",
    "                                                nn.ReLU(), \n",
    "                                                nn.Linear(2*latent_dim, latent_dim))\n",
    "\n",
    "        self.pro2lat=nn.Linear(sensor_dim, latent_dim)\n",
    "                                        \n",
    "        self.CUT=CUT\n",
    "        \n",
    "        if self.CUT:\n",
    "            self.pro2visual=nn.Linear(sensor_dim, latent_dim)\n",
    "                                        \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            self.pro2lat.weight.zero_()\n",
    "            \n",
    "            if self.CUT:\n",
    "                self.pro2visual.weight.zero_()\n",
    "        \n",
    "            for i in range(sensor_dim):\n",
    "            \n",
    "                self.pro2lat.weight[i,i]=1\n",
    "                \n",
    "                if self.CUT:\n",
    "                    self.pro2visual.weight[i,i]=1\n",
    "\n",
    "    \n",
    "    def forward(self, x, pro_b, path_visual_self, path_visual_other):\n",
    "\n",
    "        batch_size=x.size()[0]\n",
    "        x_visual = x.reshape([batch_size,x.size()[1],self.resolution,self.resolution])\n",
    "        x_visual_self = self.encoder_visual_self(x_visual)  # [B*T, C, H/8, W/8]\n",
    "        x_visual_other = self.encoder_visual_other(x_visual)\n",
    "        \n",
    "        x_visual_self = self.flatten(x_visual_self)  # [B*T, C * H/8 * W/8]\n",
    "        x_visual_other = self.flatten(x_visual_other)\n",
    "\n",
    "        z_self_pro = self.pro2lat(pro_b)\n",
    "\n",
    "        z_self_pro2visual=torch.zeros_like(z_self_pro,device=device)\n",
    "        if self.CUT:    \n",
    "            z_self_pro2visual = self.pro2visual(pro_b)\n",
    "        \n",
    "        z_self_visual=self.project_self(x_visual_self).reshape([batch_size,-1])\n",
    "        \n",
    "        z_other=self.project_other(x_visual_other).reshape([batch_size,-1])*path_visual_other\n",
    "\n",
    "        return z_self_pro, z_self_visual, z_other, z_self_pro2visual\n",
    "\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, resolution, cnn_dim, latent_dim, sensor_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        Chs=[4,16,32,64]\n",
    "        cnn_dim=Chs[-1]\n",
    "        \n",
    "        self.latent_dim=latent_dim\n",
    "        self.resolution=resolution\n",
    "\n",
    "        self.cnn_initial_size = (resolution // 4, resolution // 4)\n",
    "\n",
    "        # Latent to feature map\n",
    "        self.cnn_dim=cnn_dim\n",
    "\n",
    "        self.project_self = torch.nn.Sequential(nn.Linear(latent_dim, 2*latent_dim),\n",
    "                                                nn.ReLU(), \n",
    "                                                nn.Linear(2*latent_dim, cnn_dim * self.cnn_initial_size[0] * self.cnn_initial_size[1]))\n",
    "        \n",
    "        self.project_other = torch.nn.Sequential(nn.Linear(latent_dim, 2*latent_dim),\n",
    "                                                nn.ReLU(), \n",
    "                                                nn.Linear(2*latent_dim, cnn_dim * self.cnn_initial_size[0] * self.cnn_initial_size[1]))\n",
    "\n",
    "        self.lat2pro=torch.nn.Sequential(nn.Linear(latent_dim, latent_dim),\n",
    "                                                nn.ReLU(), \n",
    "                                                nn.Linear(latent_dim, latent_dim),\n",
    "                                                nn.ReLU(),\n",
    "                                                nn.Linear(latent_dim, sensor_dim)       \n",
    "                                        )                    \n",
    "        self.decoder_visual_self=ResNet_Transpose(Chs)\n",
    "        self.decoder_visual_other=ResNet_Transpose(Chs)\n",
    "\n",
    "        \n",
    "    def forward(self, z_self, z_other):\n",
    "        \n",
    "        B = z_self.size(0)\n",
    "\n",
    "        pro_rec=self.lat2pro(z_self)\n",
    "        \n",
    "        x_self = self.project_self(z_self)\n",
    "        x_other = self.project_other(z_other)\n",
    "        \n",
    "        x_self = x_self.view(B, self.cnn_dim, *self.cnn_initial_size)\n",
    "        x_other = x_other.view(B, self.cnn_dim, *self.cnn_initial_size)\n",
    "\n",
    "        x_self=self.decoder_visual_self(x_self.reshape([B,self.cnn_dim,*self.cnn_initial_size]))\n",
    "        x_other=self.decoder_visual_other(x_other.reshape([B,self.cnn_dim,*self.cnn_initial_size]))\n",
    "\n",
    "        \n",
    "        mask_self = torch.mean(x_self[:, 0:3:, :self.resolution, :self.resolution],1,keepdim=True) \n",
    "        mask_self = torch.sigmoid((mask_self))\n",
    "        \n",
    "        mask_other = x_other[:, -1:, :self.resolution, :self.resolution]\n",
    "        \n",
    "        mask_other = ((1 - mask_self).detach())\n",
    "        \n",
    "        x_self = x_self[:, 0:-1, :self.resolution, :self.resolution]  # Crop if needed\n",
    "        x_other = x_other[:, 0:-1, :self.resolution, :self.resolution]  # Crop if needed\n",
    "        \n",
    "        img_self = x_self * mask_self\n",
    "        img_other = x_other * mask_other\n",
    "\n",
    "        return img_self, img_other, mask_self, mask_other, pro_rec\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c85ea-049f-48b5-9090-0e94e39d34dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODE_IntMethods:\n",
    "    \n",
    "    def __init__(self,F,dt):\n",
    "        \n",
    "        self.dt=dt\n",
    "        self.F=F.to(device)\n",
    "        self.device=device\n",
    "    \n",
    "    \n",
    "    def RK2(self,X,I,t):\n",
    "        \n",
    "        k1=self.F(X,t,I)\n",
    "        k2=self.F(X+k1*self.dt,t+self.dt,I)\n",
    "        x_new=X+1/2*(k1+k2)*self.dt\n",
    "        \n",
    "        return x_new\n",
    "    \n",
    "\n",
    "    def RK4(self,X,I,t):\n",
    "        \n",
    "        k1=self.F(X,t,I)\n",
    "        k2=self.F(X+k1*self.dt/2,t+self.dt/2,I)\n",
    "        k3=self.F(X+k2*self.dt/2,t+self.dt/2,I)\n",
    "        k4=self.F(X+k3*self.dt,t+self.dt,I)\n",
    "        x_new=X+1/6*(k1+2*k2+2*k3+k4)*self.dt\n",
    "        \n",
    "        return x_new\n",
    "    \n",
    "    def Compute_Dynamics(self,Input,x0,t0):\n",
    "        \n",
    "        T=Input.size()[2]\n",
    "        batch_size=x0.size()[0]\n",
    "        N=x0.size()[1]\n",
    "        \n",
    "        X=torch.zeros([batch_size,N,T],device=device)\n",
    "        X[:,:,0]=x0\n",
    "        \n",
    "        t=t0\n",
    "        \n",
    "        for n in range(1,T):\n",
    "            \n",
    "            if Input!=[]:\n",
    "                I=Input[:,:,n]\n",
    "\n",
    "            X[:,:,n]=self.RK4(X[:,:,n-1],I,t)\n",
    "                    \n",
    "            \n",
    "        return X\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \n",
    "    def __init__(self,F_Ns):\n",
    "        super().__init__()\n",
    "                \n",
    "        module=[]\n",
    "        \n",
    "        for n in range(1,F_Ns.size()[0]):\n",
    "                        \n",
    "            module.append(nn.Linear(F_Ns[n-1],F_Ns[n]))\n",
    "        \n",
    "            if n<F_Ns.size()[0]-1:\n",
    "            \n",
    "                module.append(nn.ReLU())\n",
    "                \n",
    "        self.F=nn.Sequential(*module)\n",
    "        \n",
    "        self.F_Ns=F_Ns\n",
    "                    \n",
    "    def forward(self,X,t=[],S=[]):\n",
    "        \n",
    "        if S!=[]:\n",
    "            \n",
    "            Input=torch.concat([X,S],1)\n",
    "            \n",
    "        else:\n",
    "            Input=X\n",
    "            \n",
    "        y=self.F(Input)\n",
    "        \n",
    "        return y\n",
    "\n",
    "\n",
    "class Model_ODE(nn.Module):\n",
    "    \n",
    "    def __init__(self, F_Ns, dt):\n",
    "        super().__init__()    \n",
    "        \n",
    "        self.F_Ns=F_Ns\n",
    "        \n",
    "        self.dt=dt\n",
    "        self.f=MLP(F_Ns).to(device)\n",
    "        self.F=ODE_IntMethods(self.f,dt)\n",
    "                    \n",
    "            \n",
    "    def Reset(self,Input,t0s):\n",
    "        \n",
    "        self.X=Input\n",
    "        self.t=t0s\n",
    "        \n",
    "    def ODE_step(self,Input):\n",
    "        \n",
    "        self.X=self.F.RK2(self.X,Input,self.t)\n",
    "        self.t=self.t+self.dt\n",
    "    \n",
    "    def forward(self,Input,X0s,t0s):\n",
    "        \n",
    "        T=Input.size()[-1]\n",
    "        batch_size=Input.size()[0]\n",
    "\n",
    "        y=torch.zeros([batch_size,X0s.size()[1],T],device=device)\n",
    "        \n",
    "        err=0\n",
    "    \n",
    "        self.Reset(X0s,t0s)\n",
    "        \n",
    "        y[:,:,0]=X0s.clone()\n",
    "        \n",
    "        I=Input\n",
    "        \n",
    "        for t in range(1,T):\n",
    "            \n",
    "                \n",
    "            self.ODE_step(I[:,:,t-1])\n",
    "                            \n",
    "            y[:,:,t]=self.X.clone()\n",
    "                \n",
    "        \n",
    "        return y\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b3679-f170-4869-834d-779f8e1c3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import rgb_to_grayscale\n",
    "\n",
    "def to_grayscale_batch(x):\n",
    "    \n",
    "    \"\"\"\n",
    "    x: [B, C, H, W], C in {1, 3}. dtype uint8 or float.\n",
    "    Returns: [B, 1, H, W] float32 in [0, 1].\n",
    "    \"\"\"\n",
    "    if x.dtype == torch.uint8:\n",
    "        x = x.float() / 255.0  # normalize; keeps device (CPU/GPU)\n",
    "    if x.ndim != 4 or x.size(1) not in (1, 3):\n",
    "        raise ValueError(f\"Expected [B, C, H, W] with C=1 or 3, got {tuple(x.shape)}\")\n",
    "\n",
    "    # If already single-channel, pass through; otherwise convert RGB->gray\n",
    "    y = x if x.size(1) == 1 else rgb_to_grayscale(x)  # -> [B, 1, H, W]\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593b6b82-36b7-4e47-8700-234c2f16f052",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "        \n",
    "\n",
    "class Self_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, resolution, cnn_dim, latent_dim, sensor_dim, \\\n",
    "                 F_Ns, dt, RK=2):             ## PREDICTIVE HYPER\n",
    "        super().__init__()    \n",
    "\n",
    "        self.latent_dim=latent_dim\n",
    "        self.cnn_dim=cnn_dim\n",
    "        self.sensor_dim=sensor_dim\n",
    "        \n",
    "        ## CORRELATION\n",
    "        self.encoder=Encoder(resolution, cnn_dim, latent_dim, sensor_dim).to(device)\n",
    "        self.decoder=Decoder(resolution, cnn_dim, latent_dim, sensor_dim).to(device)\n",
    "        #self.opt=optim.Adam( list(self.encoder.parameters())+list(self.decoder.parameters()), lr=0.001)\n",
    "        \n",
    "        \n",
    "        ## PREDICTIVE\n",
    "        self.Predictive=Model_ODE(F_Ns, dt)\n",
    "        #self.opt_Predictive=optim.Adam(list(self.Predictive.F.F.parameters()), lr=0.001)\n",
    "\n",
    "        self.opt=optim.Adam( list(self.encoder.parameters())+list(self.decoder.parameters())+list(self.Predictive.F.F.parameters()), lr=0.001)\n",
    "        \n",
    "        \n",
    "    def Override_Predictive(self, F_Ns, dt, lr):\n",
    "\n",
    "        self.Predictive=Model_ODE(F_Ns, dt)\n",
    "        self.opt_Predictive=optim.Adam(list(self.Predictive.F.F.parameters())+list(self.decoder.lat2pro.parameters()), lr=0.001)\n",
    "\n",
    "    def Accuracy(self, img, lab, self_acc=True):\n",
    "\n",
    "        mask=to_grayscale_batch(img)\n",
    "        \n",
    "        b=img_self.size()[0]\n",
    "        tot_ind=128*128\n",
    "\n",
    "        if self_acc:\n",
    "            \n",
    "            lab=(lab==1).reshape([b, -1])\n",
    "            ind_sum=torch.sum( lab.float(), 1)\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            lab=(lab>1).reshape([b, -1])\n",
    "            ind_sum=torch.sum( lab.float(), 1)\n",
    "\n",
    "        m=mask.reshape([b,-1])\n",
    "        \n",
    "        ind_sort=torch.sort(m,1)[1]\n",
    "        ind_sort_lab=torch.sort(lab.float(),1)[1]\n",
    "        \n",
    "        max_k = int(ind_sum.max().item())\n",
    "        topk   = ind_sort[:, -max_k:]                                        # [B, max_k]\n",
    "        topk_lab = ind_sort_lab[:, -max_k:]\n",
    "        valid  = torch.arange(max_k, device=device).unsqueeze(0) < ind_sum.unsqueeze(1)  # [B, max_k]\n",
    "        \n",
    "        \n",
    "        mask_flat = torch.zeros(b, m.size()[1], dtype=torch.bool, device=device)   # or dtype=torch.float32\n",
    "        mask_flat_lab = torch.zeros(b, m.size()[1], dtype=torch.bool, device=device)\n",
    "        \n",
    "        rows = torch.arange(b, device=device).unsqueeze(1).expand(-1, max_k)\n",
    "        mask_flat[rows[valid], topk[valid]] = True  \n",
    "        mask_flat_lab[rows[valid], topk_lab[valid]] = True \n",
    "        \n",
    "        norm=(mask_flat_lab*lab).float().sum()/(torch.tensor([ind_sum.max()*b, mask_flat_lab.sum()]).min())\n",
    "        acc=(mask_flat*lab).float().sum()/(torch.tensor([ind_sum.max()*b, mask_flat.sum()]).min())/norm\n",
    "        \n",
    "        \n",
    "        return acc\n",
    "        \n",
    "    \n",
    "    def Correlation_forward(self,x_b, pro_b, x_b_pred, pro_b_pred, A_b_pred, x0s, t0s, n, TRAIN=True, PRO=0, eta_consistency=0.5):\n",
    "\n",
    "        batch_size=x_b.size()[0]\n",
    "\n",
    "        if n<10000:\n",
    "            \n",
    "            path_visual_self=torch.rand([batch_size,1],device=device)\n",
    "            path_visual_other=torch.zeros([batch_size,1],device=device)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            path_visual_self=torch.rand([batch_size,1],device=device)\n",
    "            path_visual_other=torch.ones([batch_size,1],device=device)\n",
    "            \n",
    "            \n",
    "        \n",
    "        #z_self_pro, z_self_visual, z_other, z_self_pro2visual=self.encoder(x_b, pro_b*0, path_visual_self, path_visual_other)\n",
    "        z_self_pro, z_self_visual, z_other, z_self_pro2visual=self.encoder(x_b, pro_b, path_visual_self, path_visual_other)\n",
    "        z_self=(1-path_visual_self)*z_self_pro+path_visual_self*z_self_visual\n",
    "        img_self, img_other, mask_self, mask_other, pro_rec = self.decoder(z_self,z_other)\n",
    "        \n",
    "        if n<10000:\n",
    "            \n",
    "            img = (img_self + img_other.detach())\n",
    "            \n",
    "        else:\n",
    "\n",
    "            img = (img_self + img_other)\n",
    "\n",
    "        consistency_loss1=torch.zeros([1], device=device)\n",
    "        \n",
    "        if PRO==1:\n",
    "            Z_, X0s=self.Correlation(x_b_pred, pro_b_pred)\n",
    "            consistency_loss1 = torch.mean(torch.pow(Z_[:,0:self.latent_dim,1:]-Z_[:,0:self.latent_dim,0:-1],2))\n",
    "        \n",
    "        recon_loss1 = torch.mean(torch.abs(img-x_b[:,0:-1,:,:]))\n",
    "\n",
    "        if self.encoder.CUT:\n",
    "            joint_loss1 = torch.mean(torch.pow(z_self_pro2visual-z_self_visual,2))\n",
    "        else:\n",
    "            joint_loss1=torch.zeros([1],device=device)\n",
    "            \n",
    "        recon_pro_loss1 = torch.mean(torch.pow(pro_rec-pro_b,2))\n",
    "        \n",
    "        err = recon_loss1  + 0.5*joint_loss1 + recon_pro_loss1 + eta_consistency*consistency_loss1\n",
    "\n",
    "        \n",
    "        if TRAIN:\n",
    "            \n",
    "            err.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "            self.opt_Predictive.zero_grad()\n",
    "\n",
    "        return recon_loss1.detach(), joint_loss1.detach(), recon_pro_loss1.detach(), consistency_loss1.detach(),\\\n",
    "        z_self.detach(), z_other.detach(), img.detach(), img_self.detach(), img_other.detach(), mask_self.detach(), mask_other.detach()\n",
    "\n",
    "\n",
    "    def Correlation_Eval(self,x_b, pro_b, x_b_pred, pro_b_pred, A_b_pred, x0s, t0s, n, lab, TRAIN=True, PRO=0):\n",
    "\n",
    "        batch_size=x_b.size()[0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            path_visual_self=torch.rand([batch_size,1],device=device)\n",
    "            \n",
    "            if n<10000:\n",
    "                path_visual_other=torch.zeros([batch_size,1],device=device)\n",
    "                path_visual_ot=0.\n",
    "            else:\n",
    "                path_visual_other=torch.ones([batch_size,1],device=device)\n",
    "                path_visual_ot=1.\n",
    "                \n",
    "            z_self_pro, z_self_visual, z_other, z_self_pro2visual=self.encoder(x_b, pro_b, path_visual_self, path_visual_other)\n",
    "            z_self_mixed=(1-path_visual_self)*z_self_pro+path_visual_self*z_self_visual\n",
    "\n",
    "            consistency_loss=torch.zeros([1], device=device)\n",
    "            if PRO==1:\n",
    "                Z_,_,_=self.Correlation(x_b_pred, pro_b_pred,n)\n",
    "                consistency_loss = torch.mean(torch.pow(Z_[:,0:self.latent_dim,1:]-Z_[:,0:self.latent_dim,0:-1],2))\n",
    "                \n",
    "        \n",
    "            img_self_mixed, img_other_mixed, mask_self_mixed, mask_other_mixed, pro_rec = self.decoder(z_self_mixed,z_other)\n",
    "            \n",
    "            img_mixed = img_self_mixed + img_other_mixed\n",
    "            \n",
    "            recon_loss_mixed = torch.mean(torch.abs(img_mixed-x_b[:,0:-1,:,:]))\n",
    "\n",
    "            acc_self_mixed=self.Accuracy(img_self_mixed,lab,self_acc=True)\n",
    "            acc_other_mixed=self.Accuracy(img_other_mixed,lab,self_acc=False)\n",
    "            \n",
    "            if self.encoder.CUT:\n",
    "                \n",
    "                joint_loss = torch.mean(torch.pow(z_self_pro2visual-z_self_visual,2))\n",
    "            \n",
    "            else:\n",
    "                \n",
    "                joint_loss=torch.zeros([1],device=device)\n",
    "            \n",
    "            recon_pro_loss = torch.mean(torch.pow(pro_rec-pro_b,2))\n",
    "    \n",
    "            \n",
    "            img_self_visual, img_other_visual, mask_self_visual, mask_other_visual, _ = self.decoder(z_self_visual,z_other)\n",
    "            img_visual = img_self_visual + img_other_visual\n",
    "    \n",
    "            recon_loss_visual = torch.mean(torch.abs(img_visual-x_b[:,0:-1,:,:]))\n",
    "\n",
    "            acc_self_visual=self.Accuracy(img_self_visual,lab,self_acc=True)\n",
    "            acc_other_visual=self.Accuracy(img_other_visual,lab,self_acc=False)\n",
    "            \n",
    "            \n",
    "            img_self_pro, img_other_pro, mask_self_pro, mask_other_pro, _ = self.decoder(z_self_pro,z_other)\n",
    "            img_pro = img_self_pro + img_other_pro\n",
    "\n",
    "            acc_self_pro=self.Accuracy(img_self_pro,lab,self_acc=True)\n",
    "            acc_other_pro=self.Accuracy(img_other_pro,lab,self_acc=False)\n",
    "            \n",
    "            recon_loss_pro = torch.mean(torch.abs(img_pro-x_b[:,0:-1,:,:]))\n",
    "    \n",
    "            cosine_pro=F.cosine_similarity(z_self_pro, z_self_mixed, dim=1).mean(0)\n",
    "            cosine_visual=F.cosine_similarity(z_self_visual, z_self_mixed, dim=1).mean(0)\n",
    "            cosine_vp=F.cosine_similarity(z_self_pro, z_self_visual, dim=1).mean(0)\n",
    "\n",
    "        \n",
    "        return recon_loss_mixed.detach(),  recon_loss_visual.detach(),  \\\n",
    "        recon_loss_pro.detach(), joint_loss.detach(), recon_pro_loss.detach(), consistency_loss.detach(), \\\n",
    "        acc_self_mixed.detach(), acc_other_mixed.detach(), acc_self_pro.detach(), acc_other_pro.detach(), acc_self_visual.detach(), acc_other_visual.detach(), \\\n",
    "        img_mixed.detach(), img_self_mixed.detach(), img_other_mixed.detach(), mask_self_mixed.detach(), mask_other_mixed.detach(), \\\n",
    "        img_visual.detach(), img_self_visual.detach(), img_other_visual.detach(), mask_self_visual.detach(), mask_other_visual.detach(), \\\n",
    "        img_pro.detach(), img_self_pro.detach(), img_other_pro.detach(), mask_self_pro.detach(), mask_other_pro.detach(), \\\n",
    "        cosine_pro.detach(), cosine_visual.detach(), cosine_vp.detach()\n",
    "            \n",
    "    \n",
    "    def Correlation(self, x_b, pro_b, n):\n",
    "        \n",
    "        T_hor=x_b.size()[1]\n",
    "        batch_size=x_b.size()[0]\n",
    "        x_b=x_b.view([batch_size*T_hor,x_b.size()[2],x_b.size(3),x_b.size(4)])\n",
    "        pro_b=pro_b.transpose(-1,-2).reshape([batch_size*T_hor,-1])\n",
    "\n",
    "        if n<10000:\n",
    "        \n",
    "            path_visual_self=torch.tile(torch.rand([batch_size,1,1],device=device),[1,T_hor,1]).reshape([batch_size*T_hor,-1])\n",
    "            path_visual_other=torch.zeros([batch_size*T_hor,1],device=device)\n",
    "\n",
    "        else:\n",
    "\n",
    "            path_visual_self=torch.tile(torch.rand([batch_size,1,1],device=device),[1,T_hor,1]).reshape([batch_size*T_hor,-1])\n",
    "            path_visual_other=torch.ones([batch_size*T_hor,1],device=device)\n",
    "\n",
    "        z_self_pro, z_self_visual, z_other, z_self_pro2visual=self.encoder(x_b, pro_b, path_visual_self, path_visual_other)\n",
    "            \n",
    "        z_=(1-path_visual_self)*z_self_pro+path_visual_self*z_self_visual\n",
    "        \n",
    "        z_self=z_.view([batch_size, T_hor, z_.size()[1]]).transpose(-1,-2)\n",
    "        z_other=z_other.view([batch_size, T_hor, z_.size()[1]]).transpose(-1,-2)\n",
    "        \n",
    "        X0s=z_.reshape([batch_size,T_hor,-1])[:,0,:]\n",
    "        \n",
    "        return z_self, z_other, X0s\n",
    "\n",
    "    def Predictive_forward(self,x_b,y_b,A_b,t0s,n,T_decode=5,TRAIN=True):\n",
    "\n",
    "        T=x_b.size()[1]\n",
    "\n",
    "        Ts=int(np.floor(T/T_decode))*torch.arange(0,5)\n",
    "        Ts[-1]=T-1\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "        #Z_self, Z_other, X0s=self.Correlation(x_b[:,Ts], y_b[:,:,Ts]*0, n)\n",
    "        ## USE THE ONE ABOVE TO SUPPRESS PROPRIOCEPTION\n",
    "        Z_self, Z_other, X0s=self.Correlation(x_b[:,Ts], y_b[:,:,Ts], n)\n",
    "        \n",
    "        Z_self_pred=self.Predictive.forward(A_b,X0s,t0s)\n",
    "        img_self=torch.zeros([1],device=device)\n",
    "\n",
    "        Z_self_pred=Z_self_pred[:,:,Ts]\n",
    "        \n",
    "        batch_size=Z_self_pred.size()[0]\n",
    "        \n",
    "        Z_self_pred=Z_self_pred.transpose(-1,-2).reshape([batch_size*T_decode, -1])\n",
    "        Z_other=Z_other.transpose(-1,-2).reshape([batch_size*T_decode, -1])\n",
    "\n",
    "        img_self, img_other, mask_self, mask_other, pro_rec = self.decoder(Z_self_pred,Z_other)\n",
    "\n",
    "        img_self=img_self.reshape([batch_size,T_decode,3,self.decoder.resolution,self.decoder.resolution])\n",
    "        img_other=img_other.reshape([batch_size,T_decode,3,self.decoder.resolution,self.decoder.resolution])\n",
    "\n",
    "        \n",
    "        if n<10000:\n",
    "            \n",
    "            img = (img_self + img_other.detach())\n",
    "            \n",
    "            target_x=x_b[:,Ts,0:-1,:,:]\n",
    "            recon_loss1 = torch.mean(torch.abs(img-target_x),[2,3,4])\n",
    "            \n",
    "            pro_b=y_b[:,:,Ts].permute(0,-1,1)\n",
    "            recon_pro_loss1 = torch.mean(torch.pow(pro_rec.reshape([batch_size,T_decode,-1])-pro_b,2),[-1])\n",
    "\n",
    "            #err = recon_loss1[:,0].mean()+recon_pro_loss1[:,0].mean()\n",
    "            err = recon_loss1.mean()+recon_pro_loss1.mean()\n",
    "            \n",
    "        else:\n",
    "\n",
    "            img_current = (img_self[:,0:1] + img_other[:,0:1])\n",
    "            img_future = (img_self[:,1:] + img_other[:,1:].detach())\n",
    "            \n",
    "            img=torch.concat([img_current,img_future],1)\n",
    "\n",
    "            target_x=x_b[:,Ts,0:-1,:,:]\n",
    "            recon_loss1 = torch.mean(torch.abs(img-target_x),[2,3,4])\n",
    "            \n",
    "            pro_b=y_b[:,:,Ts].permute(0,-1,1)\n",
    "            recon_pro_loss1 = torch.mean(torch.pow(pro_rec.reshape([batch_size,T_decode,-1])-pro_b,2),[-1])\n",
    "\n",
    "            err = recon_loss1.mean()+recon_pro_loss1.mean()\n",
    "\n",
    "\n",
    "        if TRAIN:\n",
    "            \n",
    "            err.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "\n",
    "        recon_loss1=recon_loss1.reshape([batch_size,T_decode]).mean(0)\n",
    "        recon_pro_loss1=recon_pro_loss1.reshape([batch_size,T_decode]).mean(0)\n",
    "\n",
    "        return recon_loss1.detach(), recon_pro_loss1.detach(),\\\n",
    "        Z_self_pred.detach(), Z_other.detach(), img.detach(), img_self.detach(), img_other.detach(), mask_self.detach(), mask_other.detach()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c298ae6-6357-4825-9095-d51815704d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution=128\n",
    "cnn_dim=32 \n",
    "\n",
    "latent_dim=100 \n",
    "sensor_dim=76\n",
    "\n",
    "eta=0.001\n",
    "\n",
    "X_DIM=latent_dim+latent_dim\n",
    "S_DIM=Data.A.size()[1]\n",
    "F_Ns=torch.tensor([latent_dim+A_b.size()[1],200,200,latent_dim])\n",
    "dt=0.1\n",
    "dt=torch.tensor(dt).float()\n",
    "\n",
    "self_model=Self_Model(resolution, cnn_dim, latent_dim, sensor_dim, \\\n",
    "                 F_Ns, dt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c488f8-ce09-4e2b-bdd4-c026391e308f",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_hor=50\n",
    "k=0\n",
    "X_b, Y_b, A_b, x0s, t0s=Data.Batch_ODE(batch_size, T_horizon=T_hor)\n",
    "    \n",
    "recon_loss1, pro_loss1, z_self, z_other, \\\n",
    "    img, img_self, img_other, mask_self, mask_other=self_model.Predictive_forward(X_b, Y_b, A_b, t0s, k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51b0727-4801-4b35-89e1-2d94de168497",
   "metadata": {},
   "outputs": [],
   "source": [
    "SETTINGS=[0.5]\n",
    "eta_consistency=SETTINGS[inp]\n",
    "\n",
    "import warnings, math\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "SAVE=False\n",
    "PLOT=True\n",
    "PRO=1\n",
    "\n",
    "LOAD=False\n",
    "\n",
    "if LOAD:\n",
    "\n",
    "    self_model=torch.load('../Varying_penalty/1/Model_VaryingPenalty_after_corr_0.0_Rep_1_Include_Other_False.pt')\n",
    "    #self_model=torch.load('./Model_BPTT.pt')\n",
    "    self_model.Override_Predictive(F_Ns, dt, lr=0.001)\n",
    "    \n",
    "\n",
    "N_log=100\n",
    "\n",
    "N_train1=10000\n",
    "N_checks1=np.unique(np.int32(np.exp(np.linspace(0,np.log(N_train1),N_log))))\n",
    "\n",
    "N_train2=50000\n",
    "N_checks2=np.unique(np.int32(np.exp(np.linspace(0,np.log(N_train2),N_log))))+N_train1\n",
    "N_checks=np.concatenate([N_checks1, N_checks2],0)\n",
    "N_train=N_train1+N_train2\n",
    "\n",
    "N_checks=np.arange(0,101)*600#+10000\n",
    "\n",
    "N_check=N_checks.shape[0]\n",
    "\n",
    "ind_help=0\n",
    "\n",
    "T_decode=5\n",
    "MSE_Train=torch.zeros([2,N_train,T_decode])\n",
    "MSE_Tr=torch.zeros([2,N_check,T_decode])\n",
    "\n",
    "MSE_Te=torch.zeros([2,N_check,T_decode])\n",
    "\n",
    "MSE_Te_corr=torch.zeros([6,N_check])\n",
    "ACC_Te=torch.zeros([6,N_check])\n",
    "Cosines=torch.zeros([3,N_check])\n",
    "\n",
    "batch_size=32\n",
    "ind_help=0\n",
    "\n",
    "\n",
    "for k in range(0,N_train1+N_train2):\n",
    "\n",
    "    \n",
    "    X_b, Y_b, A_b, x0s, t0s=Data.Batch_ODE(batch_size, T_horizon=50)\n",
    "    \n",
    "    recon_loss1, pro_loss1, z_self, z_other, \\\n",
    "        img, img_self, img_other, mask_self, mask_other=self_model.Predictive_forward(X_b, Y_b, A_b, t0s, n=k, T_decode=5)\n",
    "\n",
    "        \n",
    "    MSE_Train[0,k,:]=recon_loss1.detach()\n",
    "    MSE_Train[1,k,:]=pro_loss1.detach()\n",
    "   \n",
    "    if np.any(k==N_checks):\n",
    "        \n",
    "        if k>0:\n",
    "            \n",
    "            mse_mean=torch.mean(MSE_Train[:,k-N_checks[ind_help-1]:k,:],1)\n",
    "        \n",
    "            MSE_Tr[:,ind_help,:]=mse_mean\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            MSE_Tr[:,ind_help,:]=MSE_Train[:,k,:]\n",
    "        \n",
    "        x_b, y_b, lab=Data.Evaluate(batch_size*10)\n",
    "        X_te, Y_te, A_te, x0s, t0s, lab_=Data.Evaluate_ODE(batch_size, T_horizon_val=3)\n",
    "\n",
    "        recon_loss_mixed, recon_loss_visual, recon_loss_pro, joint_loss, recon_pro_loss, consistency_loss, \\\n",
    "        acc_self_mixed, acc_other_mixed, acc_self_pro, acc_other_pro, acc_self_visual, acc_other_visual, \\\n",
    "        img_mixed, img_self_mixed, img_other_mixed, mask_self_mixed, mask_other_mixed, \\\n",
    "        img_visual, img_self_visual, img_other_visual, mask_self_visual, mask_other_visual, \\\n",
    "        img_pro, img_self_pro, img_other_pro, mask_self_pro, mask_other_pro, \\\n",
    "        cosine_mixed, cosine_visual, cosine_pro = self_model.Correlation_Eval(x_b, y_b, X_te, Y_te, A_te, x0s, t0s, k, lab, TRAIN=False, PRO=PRO)\n",
    "        \n",
    "        MSE_Te_corr[0,ind_help]=recon_loss_mixed.detach()\n",
    "        MSE_Te_corr[1,ind_help]=recon_loss_visual.detach()\n",
    "        MSE_Te_corr[2,ind_help]=recon_loss_pro.detach()\n",
    "        MSE_Te_corr[3,ind_help]=joint_loss.detach()\n",
    "        MSE_Te_corr[4,ind_help]=recon_pro_loss.detach()\n",
    "        MSE_Te_corr[5,ind_help]=consistency_loss.detach()\n",
    "\n",
    "        ACC_Te[0,ind_help]=acc_self_mixed\n",
    "        ACC_Te[1,ind_help]=acc_other_mixed\n",
    "        ACC_Te[2,ind_help]=acc_self_visual\n",
    "        ACC_Te[3,ind_help]=acc_other_visual\n",
    "        ACC_Te[4,ind_help]=acc_self_pro\n",
    "        ACC_Te[5,ind_help]=acc_other_pro\n",
    "        \n",
    "        Cosines[0,ind_help]=cosine_mixed\n",
    "        Cosines[1,ind_help]=cosine_visual\n",
    "        Cosines[2,ind_help]=cosine_pro\n",
    "        \n",
    "        \n",
    "        print(k,'Err Tr: ', MSE_Tr[:,ind_help])\n",
    "        print(k,'Err Te: ', MSE_Te_corr[:,ind_help])\n",
    "        print(k,'Cosines: ', Cosines[:,ind_help])\n",
    "        print(k,'ACC: ', ACC_Te[:,ind_help])\n",
    "\n",
    "        \n",
    "        ### PREDICTIVE\n",
    "        T_hor_val=50\n",
    "        X_te, Y_te, A_te, x0s, t0s, lab_=Data.Evaluate_ODE(batch_size, T_horizon_val=T_hor_val)\n",
    "\n",
    "        with torch.no_grad(): \n",
    "            \n",
    "            recon_loss1, pro_loss1, z_self, z_other, \\\n",
    "            img, img_self, img_other, mask_self, mask_other=self_model.Predictive_forward(X_te,Y_te,A_te,t0s,n=k,TRAIN=False)\n",
    "\n",
    "        MSE_Te[0,ind_help,:]=recon_loss1.detach()\n",
    "        MSE_Te[1,ind_help,:]=pro_loss1.detach()\n",
    "        \n",
    "\n",
    "        print(k, 'PREDICTIVE')\n",
    "        print('Err Test: ', MSE_Te[0,ind_help] )\n",
    "        print('Err Test: ', MSE_Te[1,ind_help] ) \n",
    "        \n",
    "        \n",
    "        ind_help+=1\n",
    "\n",
    "        if SAVE:\n",
    "\n",
    "            title_model='Model_BPTT_NoPro_0_proLpy.pt'\n",
    "            title_model='Model_BPTT_Scaffold.pt'\n",
    "            torch.save(self_model, title_model)\n",
    "\n",
    "            title_results='Results_BPTT_Scaffold'\n",
    "            np.savez(title_results, MSE_Train=MSE_Train, MSE_Tr=MSE_Tr, MSE_Te=MSE_Te, MSE_Te_corr=MSE_Te_corr, N_checks=N_checks, Cosines=Cosines, ACC_Te=ACC_Te)\n",
    "        \n",
    "        if PLOT:\n",
    "\n",
    "            for p in range(2):\n",
    "            \n",
    "                fig, ax = plt.subplots(3, 6, figsize=(10, 5))\n",
    "    \n",
    "                ax[0,0].imshow(x_b[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[0,1].imshow(img_mixed[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[0,2].imshow(img_self_mixed[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[0,3].imshow(img_other_mixed[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[0,4].imshow(mask_self_mixed[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[0,5].imshow(mask_other_mixed[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "\n",
    "    \n",
    "                ax[1,0].imshow(x_b[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[1,1].imshow(img_visual[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[1,2].imshow(img_self_visual[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[1,3].imshow(img_other_visual[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[1,4].imshow(mask_self_visual[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[1,5].imshow(mask_other_visual[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "\n",
    "\n",
    "                ax[2,0].imshow(x_b[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[2,1].imshow(img_pro[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[2,2].imshow(img_self_pro[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[2,3].imshow(img_other_pro[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[2,4].imshow(mask_self_pro[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "                ax[2,5].imshow(mask_other_pro[p,0:3].detach().permute(1,2,0).to('cpu'))\n",
    "    \n",
    "                plt.show()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcda496-a002-4281-af61-36fd2f1d0094",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_model='Model_BPTT.pt'\n",
    "torch.save(self_model, title_model)\n",
    "\n",
    "title_results='Results_BPTT'\n",
    "np.savez(title_results, MSE_Train=MSE_Train, MSE_Tr=MSE_Tr, MSE_Te=MSE_Te, MSE_Te_corr=MSE_Te_corr, N_checks=N_checks, Cosines=Cosines, ACC_Te=ACC_Te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d20f96-d69e-4c43-9291-dfc2f14f09fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#title_model='Model_BPTT_NoPro_0Lp.pt'\n",
    "#torch.save(self_model, title_model)\n",
    "\n",
    "#title_results='Results_BPTT_NoPro_0Lp'\n",
    "#np.savez(title_results, MSE_Train=MSE_Train, MSE_Tr=MSE_Tr, MSE_Te=MSE_Te, MSE_Te_corr=MSE_Te_corr, N_checks=N_checks, Cosines=Cosines, ACC_Te=ACC_Te)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fede6530-58e4-4c4c-a9d9-e11ba094bc85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
